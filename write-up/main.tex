\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{style}

\title{Code-breaking Neural Networks}
\author{Tristan Hodgson}
\date{\today}

\begin{document}
\maketitle

\section{Motivation}

During this project, I investigate the ability of neural networks to learn to decrypt cipher text. While this is is not possible for general encryption schemes (e.g. AES and RSA), if we restrict ourselves to less cryptographically secure ciphers, it may be possible for neural networks to learn the underlying transformation from examples alone.

\section{Proof of Concept}

As a first proof of concept, we focus on the Caesar cipher, this is because it has a finite space of possible keys so we can use a classification model to predict the shift value directly, rather than requiring a more complex seq2seq model. This serves as a useful test of feasibility before we move onto more complex tasks, however it is not generalisable to other ciphers as they have much larger key spaces: even a substitution cipher has $26!$ possible keys!

In this proof of concept, we use a basic 1D CNN architecture to classify the shift value used in a Caesar cipher. The model is trained on the same Guttenberg text dataset used in the main project (see \label{sec:data}). The text is preprocessed by applying a random Caesar shift to each sequence, and the model is trained on pairs of $(\text{ciphertext}, \text{shift value})$.

Even with very little data and training time we find that the model achieves a very high level of accuracy (100\% to 4s.f. after one epoch) on the validation data set. This is a good sign for the feasibility of using neural networks to learn to decrypt ciphers, and suggests that more complex architectures could be used to tackle more complex ciphers in future work.

\section{Data}

All data is gathered from the Project Guttenberg\cite{projectgutenberg} corpus of public domain books. Text is preprocessed by converting to lowercase and removing all non-alphabetic characters. Aside from the proof of concept described above, we train using pairs of $(\text{ciphertext}, \text{plaintext})$ generated by applying a cipher to the plaintext. The ciphers we consider in this project are the Caesar cipher, the Substitution cipher and finally the Enigma machine.



\section{Model}

We experiment with two seq2seq architectures that have been used in the literature: a 1D Convolutional Neural Network (CNN) and a recurrent network based on Long Short-Term Memory (LSTM) units.

The 1D CNN architecture is motivated by its success in ciphertext classification and pattern detection tasks \cite{ahmadzadeh2022blstmgru}. Convolutional filters can act as detectors for short-range structure, similar to $n$-gram methods traditionally used in classical cryptanalysis. For simple ciphers such as Caesar and substitution ciphers, this inductive bias is often sufficient to uncover key-dependent patterns \cite{focardi2018neural}. Our CNN uses filters of sizes 2,3,5,7 chosen to capture common $n$-gram length in English text.

The second architecture is an LSTM-based recurrent neural network\cite{greydanus2017enigma}. This model is a replication of a model demonstrated by Greydanus to learn decryption of ciphers including Enigma. What is new in our work is the way in which we train this model on a large corpus of natural language text, rather than providing the key alongside the ciphertext. Unlike CNN models, the LSTM is stateful, allowing it to capture dependencies from across the cipher text. This is of particular importance for Enigma messages due to the relatively high complexity of the underlying transformation.

Training is supervised: the model receives ciphertext sequences and must predict the corresponding plaintext tokens. We train on pairs $(\text{ciphertext}, \text{plaintext})$, feeding character embeddings into the model and predicting the next plaintext character at each timestep until we predict the end of string character.


\section{Results}


\section{Future Work}

Several extensions could build on this proof of concept:

\begin{itemize}
    \item Trying adding a fixed beginning to the plaintext to see if the model can learn to decrypt general enigma messages given this fixed encrypted header (similar to adding the key as in \cite{greydanus2017enigma})
    \item Trying different languages (e.g. German) to see if the results are language dependent
    \item Trying mixed ciphers, i.e. randomly switching between two ciphers for each character in the plaintext
\end{itemize}

These extensions would help determine how far neural approaches can be pushed
into genuine cipher analysis and more sophisticated symbolic tasks.


\bibliographystyle{plain}
\bibliography{references}

\end{document}
