{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1516160",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10adae3f",
   "metadata": {},
   "source": [
    "The first attempt at using an LSTM model to decrypt data in a Seq2Seq manner is clearly not working very well, while training loss is decreasing very slowly there is little to no change to the testing loss, suggesting overfitting. This is because the Seq2Seq method is much harder to train than the classification method hence we expect to need more data and more training, one of the significant difficulties in this regard is how slow the model it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd030ff5",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eea425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 27\n",
    "SOS_TOKEN = 29\n",
    "EOS_TOKEN = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78767f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_AMOUNT = [50000, 150]\n",
    "FIXED_LENGTH = int(DATA_AMOUNT[1]*5)\n",
    "TRAIN_SPLIT = 0.8\n",
    "STREAM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "709f8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "LAYERS = 2\n",
    "ENCODE_DROPOUT, DECODE_DROPOUT = (0.5, 0.5)\n",
    "VOCAB_SIZE = 30\n",
    "NUM_EPOCHS = 5\n",
    "LR = 0.001\n",
    "CLIP = 1.0\n",
    "TEACHER_FORCING_RATIO = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa298ba9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28218d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b75d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import lstm_data\n",
    "from modules import caesar\n",
    "from modules import get_text\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b19db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea2527ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8202e",
   "metadata": {},
   "source": [
    "## Creating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c855d7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87af738ae064c97bbf1819b58030e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99842879b4304635be194d400661636e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f51e33204b48e3aa8dd3d95e1d66ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encryption = lambda x: caesar.encrypt(x, key=14)[0]\n",
    "\n",
    "trainData, testData = lstm_data.initialise(encryption, *DATA_AMOUNT, TRAIN_SPLIT, stream=STREAM, fixed_length=FIXED_LENGTH )\n",
    "train_loader, test_loader = lstm_data.data2loader(trainData, testData, BATCH_SIZE=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc1c9a",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "The model used borrows heavily from the [PyTorch Seq2Seq tutorial](https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c81bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, n_layers=LAYERS, dropout=ENCODE_DROPOUT):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        embedded = self.dropout(self.embedding(X))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c9c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, n_layers=LAYERS, dropout=DECODE_DROPOUT):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "debd866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c834c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Seq2Seq(\n",
       "    (encoder): Encoder(\n",
       "      (embedding): Embedding(30, 64)\n",
       "      (rnn): LSTM(64, 64, num_layers=2, dropout=0.5)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (embedding): Embedding(30, 64)\n",
       "      (rnn): LSTM(64, 64, num_layers=2, dropout=0.5)\n",
       "      (fc_out): Linear(in_features=64, out_features=30, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(Encoder(), Decoder(), device).to(device)\n",
    "if os.name != \"nt\":\n",
    "    model = torch.compile(model)\n",
    "    print(\"compiled\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266f95d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Seq2Seq(\n",
       "    (encoder): Encoder(\n",
       "      (embedding): Embedding(30, 64)\n",
       "      (rnn): LSTM(64, 64, num_layers=2, dropout=0.5)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (embedding): Embedding(30, 64)\n",
       "      (rnn): LSTM(64, 64, num_layers=2, dropout=0.5)\n",
       "      (fc_out): Linear(in_features=64, out_features=30, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698814db",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f5825a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 138,910 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38fb31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, clip, teacher_forcing_ratio, data_loader=train_loader, optimiser=optimiser, loss_fn=loss_fn,  device=device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        X, y = X.to(device).permute(1, 0), y.to(device).permute(1, 0)\n",
    "        optimiser.zero_grad()\n",
    "        output = model(X, y, teacher_forcing_ratio)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].contiguous().view(-1, output_dim)\n",
    "        y = y[1:].contiguous().view(-1)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimiser.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "033e20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader=test_loader, loss_fn=loss_fn, device=device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, (X, y) in enumerate(data_loader):\n",
    "            X, y = X.to(device).permute(1, 0), y.to(device).permute(1, 0)\n",
    "            output = model(X, y, 0)  # turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].contiguous().view(-1, output_dim)\n",
    "            y = y[1:].contiguous().view(-1)\n",
    "            loss = loss_fn(output, y)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd931bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0af04333324331b9670c79b33839bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.603 | Test Loss 2.854\n",
      "Train Loss: 2.579 | Test Loss 2.853\n",
      "Train Loss: 2.565 | Test Loss 2.854\n",
      "Train Loss: 2.554 | Test Loss 2.852\n",
      "Train Loss: 2.546 | Test Loss 2.852\n",
      "Train Loss: 2.541 | Test Loss 2.853\n",
      "Train Loss: 2.536 | Test Loss 2.979\n",
      "Train Loss: 2.533 | Test Loss 2.856\n",
      "Train Loss: 2.528 | Test Loss 2.850\n",
      "Train Loss: 2.522 | Test Loss 2.854\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    train_loss = train_fn(\n",
    "        model=model,\n",
    "        clip=CLIP,\n",
    "        teacher_forcing_ratio=TEACHER_FORCING_RATIO, \n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.3f} | Test Loss {valid_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f494e3b",
   "metadata": {},
   "source": [
    "# Example useage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74a78c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 'The quick brown fox jumps over the lazy dog'\n",
      "Encrypted Text: 'hvs eiwqy pfckb tcl xiadg cjsf hvs zonm rcu'\n",
      "tensor([ 0, 20,  8,  5,  0,  0,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "Model Prediction (Text):  the  ae                                     \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_string = \"The quick brown fox jumps over the lazy dog\"\n",
    "true_key = 14\n",
    "enc_text, _ = caesar.encrypt(test_string, key=true_key)\n",
    "print(f\"Original Text: '{test_string}'\")\n",
    "print(f\"Encrypted Text: '{enc_text}'\")\n",
    "\n",
    "input_tensor = torch.tensor([SOS_TOKEN] + get_text.string2_num_list(enc_text) + [EOS_TOKEN], dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    trg_tensor = torch.zeros(len(input_tensor), 1, dtype=torch.long).to(device)\n",
    "    output = model(input_tensor, trg_tensor, 0) # teacher_forcing_ratio = 0\n",
    "predicted_indexes = output.argmax(2).squeeze(1)\n",
    "predicted_chars = []\n",
    "print(predicted_indexes)\n",
    "for idx in predicted_indexes:\n",
    "    if idx.item() == EOS_TOKEN:\n",
    "        predicted_chars.append(\"@\")\n",
    "    if idx.item() == 0:\n",
    "        predicted_chars.append(\" \")\n",
    "    else:\n",
    "        predicted_chars.append(chr(idx.item() -1 + ord('a')))\n",
    "\n",
    "predicted_text = \"\".join(predicted_chars)\n",
    "print(f\"Model Prediction (Text): {predicted_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f08e6",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29af280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj=model.state_dict(), f=\"models/01_LSTM_attempt_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a470b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-breaking-nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
